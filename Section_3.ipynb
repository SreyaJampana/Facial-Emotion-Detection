{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SreyaJampana/Facial-Emotion-Detection/blob/main/Section_3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qhr-G_92H2Bc",
        "outputId": "fc63cbba-3627-4b07-dbdb-2b54c2c38c76",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "\n",
        "import cv2\n",
        "import dlib\n",
        "import pickle\n",
        "import warnings\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import itertools\n",
        "\n",
        "import urllib.request\n",
        "\n",
        "from sklearn import metrics\n",
        "from scipy.spatial import distance\n",
        "from sklearn.metrics import accuracy_score\n",
        "from matplotlib import pyplot as plt\n",
        "from tqdm import tqdm,tqdm_pandas\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "\n",
        "import re\n",
        "import keras\n",
        "\n",
        "from keras.models import Sequential\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.optimizers import Adam, SGD\n",
        "from keras.layers import Dense, Dropout, Activation, Flatten\n",
        "from keras.layers import Conv2D, MaxPooling2D, BatchNormalization\n",
        "from keras.losses import categorical_crossentropy\n",
        "from keras.regularizers import l2\n",
        "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "from keras.models import load_model\n",
        "\n",
        "# grab tools from our tensorflow and keras toolboxes!\n",
        "import tensorflow as tf\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Activation, Dropout, Flatten, Dense\n",
        "from keras import optimizers\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "def model_to_string(model):\n",
        "    import re\n",
        "    stringlist = []\n",
        "    model.summary(print_fn=lambda x: stringlist.append(x))\n",
        "    sms = \"\\n\".join(stringlist)\n",
        "    sms = re.sub('_\\d\\d\\d','', sms)\n",
        "    sms = re.sub('_\\d\\d','', sms)\n",
        "    sms = re.sub('_\\d','', sms)\n",
        "    return sms\n",
        "\n",
        "###Getting the csv data loaded\n",
        "!wget -q --show-progress \"https://storage.googleapis.com/inspirit-ai-data-bucket-1/Data/AI%20Scholars/Sessions%206%20-%2010%20(Projects)/Project%20-%20Emotion%20Detection/fer2013_5.csv\"\n",
        "\n",
        "###Getting the Dlib Shape predictor!\n",
        "!wget -q --show-progress \"https://storage.googleapis.com/inspirit-ai-data-bucket-1/Data/AI%20Scholars/Sessions%206%20-%2010%20(Projects)/Project%20-%20Emotion%20Detection/shape_predictor_68_face_landmarks.dat\"\n",
        "\n",
        "###Getting the Xpure loaded\n",
        "!wget -q --show-progress \"https://storage.googleapis.com/inspirit-ai-data-bucket-1/Data/AI%20Scholars/Sessions%206%20-%2010%20(Projects)/Project%20-%20Emotion%20Detection/pureX.npy\"\n",
        "\n",
        "###Getting the Xdata loaded\n",
        "!wget -q --show-progress \"https://storage.googleapis.com/inspirit-ai-data-bucket-1/Data/AI%20Scholars/Sessions%206%20-%2010%20(Projects)/Project%20-%20Emotion%20Detection/dataX.npy\"\n",
        "\n",
        "###Getting the Ydata loaded\n",
        "!wget -q --show-progress \"https://storage.googleapis.com/inspirit-ai-data-bucket-1/Data/AI%20Scholars/Sessions%206%20-%2010%20(Projects)/Project%20-%20Emotion%20Detection/dataY.npy\"\n",
        "\n",
        "print (\"Data Downloaded!\")\n",
        "\n",
        "'''\n",
        "Plots the confusion Matrix and saves it\n",
        "'''\n",
        "def plot_confusion_matrix(y_true,y_predicted):\n",
        "  cm = metrics.confusion_matrix(y_true, y_predicted)\n",
        "  print (\"Plotting the Confusion Matrix\")\n",
        "  labels = list(label_map.values())\n",
        "  df_cm = pd.DataFrame(cm,index = labels,columns = labels)\n",
        "  fig = plt.figure()\n",
        "  res = sns.heatmap(df_cm, annot=True,cmap='Blues', fmt='g')\n",
        "  plt.yticks([0.5,1.5,2.5,3.5,4.5], labels,va='center')\n",
        "  plt.title('Confusion Matrix - TestData')\n",
        "  plt.ylabel('True label')\n",
        "  plt.xlabel('Predicted label')\n",
        "\n",
        "  plt.show()\n",
        "  plt.close()\n",
        "\n",
        "def plot_graphs(history, best):\n",
        "\n",
        "  plt.figure(figsize=[10,4])\n",
        "  # summarize history for accuracy\n",
        "  plt.subplot(121)\n",
        "  plt.plot(history.history['accuracy'])\n",
        "  plt.plot(history.history['val_accuracy'])\n",
        "  plt.title('model accuracy across training\\n best accuracy of %.02f'%best[1])\n",
        "  plt.ylabel('accuracy')\n",
        "  plt.xlabel('epoch')\n",
        "  plt.legend(['train', 'test'], loc='upper left')\n",
        "\n",
        "  # summarize history for loss\n",
        "  plt.subplot(122)\n",
        "  plt.plot(history.history['loss'])\n",
        "  plt.plot(history.history['val_loss'])\n",
        "  plt.title('model loss across training\\n best loss of %.02f'%best[0])\n",
        "  plt.ylabel('loss')\n",
        "  plt.xlabel('epoch')\n",
        "  plt.legend(['train', 'test'], loc='upper left')\n",
        "  plt.show()\n",
        "\n",
        "#Integer to Label Mapping\n",
        "label_map = {\"0\":\"ANGRY\",\"1\":\"HAPPY\",\"2\":\"SAD\",\"3\":\"SURPRISE\",\"4\":\"NEUTRAL\"}\n",
        "\n",
        "\n",
        "#Load the 68 face Landmark file\n",
        "predictor = dlib.shape_predictor('./shape_predictor_68_face_landmarks.dat')\n",
        "\"\"\"\n",
        "Returns facial landmarks for the given input image path\n",
        "\"\"\"\n",
        "def get_landmarks(image):\n",
        "\n",
        "\n",
        "  #:type image : cv2 object\n",
        "  #:rtype landmarks : list of tuples where each tuple represents\n",
        "  #                  the x and y co-ordinates of facial keypoints\n",
        "\n",
        "  #Bounding Box co-ordinates around the face(Training data is 48*48(cropped faces))\n",
        "  rects = [dlib.rectangle(left=1, top=1, right=47, bottom=47)]\n",
        "\n",
        "  #Read Image using OpenCV\n",
        "  #image = cv2.imread(image_path)\n",
        "  #Detect the Faces within the image\n",
        "  landmarks = [(p.x, p.y) for p in predictor(image, rects[0]).parts()]\n",
        "  return image,landmarks\n",
        "\n",
        "\"\"\"\n",
        "Display image with its Facial Landmarks\n",
        "\"\"\"\n",
        "def image_landmarks(image,face_landmarks):\n",
        "  \"\"\"\n",
        "  :type image_path : str\n",
        "  :type face_landmarks : list of tuples where each tuple represents\n",
        "                     the x and y co-ordinates of facial keypoints\n",
        "  :rtype : None\n",
        "  \"\"\"\n",
        "  radius = -4\n",
        "  circle_thickness = 1\n",
        "  image_copy = image.copy()\n",
        "  for (x, y) in face_landmarks:\n",
        "    cv2.circle(image_copy, (x, y), circle_thickness, (255,0,0), radius)\n",
        "\n",
        "  plt.imshow(image_copy, interpolation='nearest')\n",
        "  plt.show()\n",
        "\n",
        "\"\"\"\n",
        "Computes euclidean distance between 68 Landmark Points for our features\n",
        "e_dist is a list of features that will go into our model.\n",
        "Each feature is a distance between two landmark points, and every pair of points\n",
        "must have a feature.\n",
        "\"\"\"\n",
        "\n",
        "def landmarks_edist(face_landmarks):\n",
        "    e_dist = []\n",
        "    for i,j  in itertools.combinations(range(68), 2):\n",
        "      e_dist.append(distance.euclidean(face_landmarks[i],face_landmarks[j]))\n",
        "    return e_dist\n",
        "\n",
        "def compare_learning(mlp, lm, cnn, vgg): # there's one model missing: MLP from pixels\n",
        "\n",
        "  # summarize history for accuracy\n",
        "  plt.plot(vgg.history['val_accuracy'],)\n",
        "  plt.plot(cnn.history['val_accuracy'])\n",
        "  plt.plot(mlp.history['val_accuracy'],)\n",
        "  plt.plot(lm.history['val_accuracy'])\n",
        "  plt.ylabel('validitation accuracy')\n",
        "  plt.xlabel('epoch')\n",
        "  plt.legend(['cnn_transfer', 'cnn_scratch', 'mlp_pixels', 'mlp_landmarks'], bbox_to_anchor=[1,1])\n",
        "  plt.xticks(range(0, epochs+1, 5), range(0, epochs+1, 5))\n",
        "  plt.show()\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fer2013_5.csv.1     100%[===================>] 159.97M  41.4MB/s    in 4.5s    \n",
            "shape_predictor_68_ 100%[===================>]  95.08M  32.7MB/s    in 2.9s    \n",
            "pureX.npy.1         100%[===================>]  43.95M  26.6MB/s    in 1.7s    \n",
            "dataX.npy.1         100%[===================>] 347.59M  13.2MB/s    in 26s     \n",
            "dataY.npy.1         100%[===================>] 156.38K   636KB/s    in 0.2s    \n",
            "Data Downloaded!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oG6SDDQIk9PG"
      },
      "source": [
        "# First, we initialize our model\n",
        "tmp_model = Sequential()\n",
        "\n",
        "# then we add a \"Dense\" (i.e. fully connected) layer\n",
        "tmp_model.add(Dense(7, input_shape=(5,), activation = 'relu'))\n",
        "\n",
        "# then we have to add another layer\n",
        "tmp_model.add(Dense(7, activation = 'relu'))\n",
        "\n",
        "# we end by defining the output layer, which has the number of dimensions of the predictions we're making\n",
        "tmp_model.add(Dense(4, activation = 'softmax'))\n",
        "\n",
        "# we finalize the model by \"compiling\" it and defining some other hyperparameters\n",
        "tmp_model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Run this to test if your model is right!\n",
        "# First, we initialize our model\n",
        "tmp_model_answer = Sequential()\n",
        "\n",
        "# then we add a \"Dense\" (i.e. fully connected) layer\n",
        "tmp_model_answer.add(Dense(7, input_shape=(5,), activation = 'relu')) # for the first layer we specify the input dimensions\n",
        "\n",
        "# then we have to add another layer\n",
        "tmp_model_answer.add(Dense(7, activation = 'relu'))\n",
        "\n",
        "# we end by defining the output layer, which has the number of dimensions of the predictions we're making\n",
        "tmp_model_answer.add(Dense(4, activation = 'softmax'))\n",
        "\n",
        "# we finalize the model by \"compiling\" it and defining some other hyperparameters\n",
        "tmp_model_answer.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "\n",
        "tmp_model_config = tmp_model.get_config()\n",
        "\n",
        "del tmp_model_config[\"name\"]\n",
        "for layer in tmp_model_config[\"layers\"]:\n",
        "  del layer[\"config\"][\"name\"]\n",
        "\n",
        "tmp_model_answer_config = tmp_model_answer.get_config()\n",
        "\n",
        "del tmp_model_answer_config[\"name\"]\n",
        "for layer in tmp_model_answer_config[\"layers\"]:\n",
        "  del layer[\"config\"][\"name\"]\n",
        "\n",
        "if tmp_model_answer_config == tmp_model_config:\n",
        "  print('Good job! Your model worked')\n",
        "else:\n",
        "  print('Please check your code again!')"
      ],
      "metadata": {
        "id": "xy8TDUqWeuLH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-rrqPkJWVLOY"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sv3U4FxbVyfs"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WJh4klMtV3Q_"
      },
      "source": [
        "### YOUR CODE HERE\n",
        "perceptron = Sequential()\n",
        "perceptron.add(Dense(1024, input_shape=(2278,), activation= 'relu', kernel_initializer='glorot_normal'))\n",
        "perceptron.add(Dense(512, activation= 'relu', kernel_initializer='glorot_normal'))\n",
        "perceptron.add(Dense(5, activation= 'softmax'))\n",
        "perceptron.compile(loss='categorical_crossentropy', optimizer=SGD(learning_rate=0.001), metrics=['accuracy'])\n",
        "### END CODE"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rWDzNTh9Wm0e"
      },
      "source": [
        "#@title Run this to test if your model is right! { display-mode: \"form\" }\n",
        "perceptron_answer = Sequential()\n",
        "perceptron_answer.add(Dense(units = 1024, input_shape = (2278,),kernel_initializer='glorot_normal',activation = 'relu'))\n",
        "perceptron_answer.add(Dense(units = 512,kernel_initializer='glorot_normal' , activation = 'relu'))\n",
        "perceptron_answer.add(Dense(units = 5, activation = 'softmax'))\n",
        "\n",
        "perceptron_answer.compile(loss='categorical_crossentropy', optimizer=SGD(learning_rate=0.001), metrics=['accuracy'])\n",
        "\n",
        "if model_to_string(perceptron) == model_to_string(perceptron_answer):\n",
        "  print('Good job, you specified it correctly!')\n",
        "else:\n",
        "  print('Please check your code again!')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oeb1iFnZw3W1"
      },
      "source": [
        "#Applying Neural Networks (MLPs) to predicting emotions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G-hJkIeQthKl"
      },
      "source": [
        "##Activity: Train Neural Network on Emotion Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LDZqO4EMHeSZ"
      },
      "source": [
        "###Set some hyper parameters for all models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "284_6dNsHiS5"
      },
      "source": [
        "# the number of times we pass all the training data through the model\n",
        "epochs = 20\n",
        "# the number of examples we pass to the model at each time\n",
        "batch_size = 16\n",
        "# the proportion of testing data we set aside (e.g. 10%)\n",
        "test_ratio = .1\n",
        "# the number of emotion categories we have to predict\n",
        "n_labels = 5"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oG7AIjf3xWBa"
      },
      "source": [
        "# load data\n",
        "dataX_pixels = np.load('pureX.npy')\n",
        "dataY_labels = np.load('dataY.npy')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qA_0acGq9AJN"
      },
      "source": [
        "# convert labels to one hot encoding\n",
        "y_onehot = to_categorical(dataY_labels, len(set(dataY_labels)))\n",
        "\n",
        "# what does this data type look like?"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cVO4wnTY7rRk"
      },
      "source": [
        "# split Data into Train, Test (90-10)\n",
        "X_train, X_test, y_train, y_test = train_test_split(dataX_pixels, y_onehot, test_size=test_ratio, random_state=42)\n",
        "\n",
        "#### Standardize the data ##########\n",
        "pixel_scaler = StandardScaler()\n",
        "pixel_scaler.fit(X_train)\n",
        "X_train = pixel_scaler.transform(X_train)\n",
        "X_test = pixel_scaler.transform(X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IwYofcaXzWVa"
      },
      "source": [
        "# Compiling the model with SGD optimizer and categorical crossentropy loss\n",
        "mlp_model.compile(loss=categorical_crossentropy, optimizer=SGD(learning_rate=0.001), metrics=['accuracy'])\n",
        "\n",
        "#Saves the Best Model Based on Val Loss\n",
        "checkpoint = ModelCheckpoint('best_mlp_model.h5', verbose=1, monitor='val_loss', save_best_only=True,  mode='auto')\n",
        "\n",
        "#training the model\n",
        "mlp_history = mlp_model.fit(X_train, y_train, batch_size=batch_size, epochs=epochs, verbose=1,\n",
        "                            callbacks=[checkpoint], validation_data=(X_test, y_test), shuffle=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qF0MmGV8Lm7y"
      },
      "source": [
        "##Neural Network Model Evaluation on pixel inputs\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ORXi0y71t3o8"
      },
      "source": [
        "###Evaluate best model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DnjfotXoth4e"
      },
      "source": [
        "mlp_performance = mlp_model.evaluate(X_test, y_test, batch_size=64)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3vgEP7OnGlR9"
      },
      "source": [
        "###Visualize accuracy and loss over training + display best model's performance"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ktHNwzn0G73y"
      },
      "source": [
        "plot_graphs(mlp_history, mlp_performance);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oPeyL20OFzjN"
      },
      "source": [
        "###Plot the Confusion Matrix"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MgkxPwg2F7eL"
      },
      "source": [
        "y_pred_mlp = mlp_model.predict(X_test)\n",
        "y_pred_mlp_classes = np.argmax(y_pred_mlp, axis=1)\n",
        "y_true = np.argmax(y_test,axis=1)\n",
        "plot_confusion_matrix(y_true, y_pred_mlp_classes)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k-aKmK6e0a4J"
      },
      "source": [
        "#Load the data (Distances between facial Landmarks)\n",
        "dataX_lm = np.load('./dataX.npy')\n",
        "\n",
        "# split Data into Train, Test (90-10)\n",
        "X_train_lm, X_test_lm, y_train_lm, y_test_lm = train_test_split(dataX_lm, y_onehot, test_size=0.1, random_state=42)\n",
        "\n",
        "#### Standardize the data #####\n",
        "lm_scaler = StandardScaler()\n",
        "lm_scaler.fit(X_train_lm)\n",
        "X_train_lm = lm_scaler.transform(X_train_lm)\n",
        "X_test_lm = lm_scaler.transform(X_test_lm)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OPub4NX31R3q"
      },
      "source": [
        "lm_model = Sequential()\n",
        "lm_model.add(Dense(1024, input_shape=(5,), activation = 'relu', kernel_initializer='glorot_normal'))\n",
        "lm_model.add(Dense(512, activation= 'relu', kernel_initializer='glorot_normal'))\n",
        "lm_model.add(Dense(70, activation= 'relu', kernel_initializer='glorot_normal'))\n",
        "lm_model.add(Dense(5, activation= 'softmax'))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_QEUcTxh7Ara"
      },
      "source": [
        "# Compiling the model with SGD optimizer and categorical crossentropy loss\n",
        "lm_model.compile(loss=categorical_crossentropy, optimizer=SGD(learning_rate=0.001), metrics=['accuracy'])\n",
        "\n",
        "#Saves the Best Model Based on Val Loss\n",
        "checkpoint = ModelCheckpoint('best_lm_model.h5', verbose=1, monitor='val_loss', save_best_only=True,  mode='auto')\n",
        "#training the model\n",
        "lm_history = lm_model.fit(X_train_lm, y_train_lm, batch_size=batch_size, epochs=epochs,\n",
        "                          verbose=1, callbacks=[checkpoint], validation_data=(X_test_lm, y_test_lm), shuffle=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lX9zvfvrAtrj"
      },
      "source": [
        "#Convolutional Neural Networks for Emotion Detection!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jXuDr1x1CZYP"
      },
      "source": [
        "###Model Hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OvPqbBy8Cc1G"
      },
      "source": [
        "# we'll use the same epochs and batch size as above\n",
        "width, height = 48, 48"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wTH-IlrL0HVE"
      },
      "source": [
        "###Reshape the inputs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DwTlyCBk6FOd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3a6298ca-5061-4df6-b600-dfe2acbd4f7b"
      },
      "source": [
        "# pixels were vectors\n",
        "print(X_train.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(18000, 2304)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NNSP1-vz6Lnk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "254bfacb-de24-4f49-a67d-683712085216"
      },
      "source": [
        "X_train_cnn = X_train.reshape(len(X_train),height,width)\n",
        "X_test_cnn = X_test.reshape(len(X_test),height,width)\n",
        "\n",
        "# we've converted them to images\n",
        "print(X_train_cnn.shape)\n",
        "# now we add one more dimension for model compatibility\n",
        "print(X_test_cnn.shape)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(18000, 48, 48)\n",
            "(2000, 48, 48)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RFP5S57i69o7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b6f52239-db9b-459e-c070-931cf54d1632"
      },
      "source": [
        "# now we add one more dimension for model compatibility\n",
        "X_train_cnn = np.expand_dims(X_train_cnn,3)\n",
        "X_test_cnn = np.expand_dims(X_test_cnn,3)\n",
        "\n",
        "print(X_train_cnn.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(18000, 48, 48, 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Build model"
      ],
      "metadata": {
        "id": "RkGSeDG9vwho"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fnI75Z1yDylH",
        "cellView": "both"
      },
      "source": [
        "# initialize model\n",
        "cnn_model = Sequential()\n",
        "# this conv layer has 64 filters! the input shape needs to be the same dimensions of the image\n",
        "cnn_model.add(Conv2D(64, kernel_size=(3, 3), activation='relu', input_shape=(width, height, 1)))\n",
        "# batch normalization\n",
        "cnn_model.add(BatchNormalization())\n",
        "# max pooling\n",
        "cnn_model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
        "# dropout\n",
        "cnn_model.add(Dropout(0.5))\n",
        "\n",
        "cnn_model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
        "\n",
        "cnn_model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
        "\n",
        "\n",
        "# flatten all the outputs between convolutional and dense layers\n",
        "cnn_model.add(Flatten())\n",
        "# add a \"dense layer\" (i.e. the fully connected layers in MLPs) with dropout\n",
        "cnn_model.add(Dense(512, activation='relu'))\n",
        "# output layer\n",
        "cnn_model.add(Dense(n_labels, activation='softmax'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YqirnWL8wDGR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2db155d4-bb20-4eaf-f8ee-482353123dcc"
      },
      "source": [
        "#Saves the Best Model Based on Val Loss\n",
        "checkpoint = ModelCheckpoint('best_cnn_model.h5', verbose=1, monitor='val_loss',save_best_only=True, mode='auto')\n",
        "\n",
        "# compliling the model with adam optimizer and categorical crossentropy loss\n",
        "cnn_model.compile(loss=categorical_crossentropy, optimizer=Adam(learning_rate=0.001, beta_1=0.9, beta_2=0.999), metrics=['accuracy'])\n",
        "\n",
        "# training the model\n",
        "cnn_history = cnn_model.fit(X_train_cnn, y_train, batch_size=batch_size, epochs=epochs, verbose=1,\n",
        "                            callbacks=[checkpoint], validation_data=(X_test_cnn, y_test), shuffle=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "1125/1125 [==============================] - ETA: 0s - loss: 1.6768 - accuracy: 0.4029\n",
            "Epoch 1: val_loss improved from inf to 1.30916, saving model to best_cnn_model.h5\n",
            "1125/1125 [==============================] - 17s 10ms/step - loss: 1.6768 - accuracy: 0.4029 - val_loss: 1.3092 - val_accuracy: 0.4580\n",
            "Epoch 2/20\n",
            "1124/1125 [============================>.] - ETA: 0s - loss: 1.2237 - accuracy: 0.4963\n",
            "Epoch 2: val_loss improved from 1.30916 to 1.28549, saving model to best_cnn_model.h5\n",
            "1125/1125 [==============================] - 11s 10ms/step - loss: 1.2240 - accuracy: 0.4961 - val_loss: 1.2855 - val_accuracy: 0.4755\n",
            "Epoch 3/20\n",
            "1124/1125 [============================>.] - ETA: 0s - loss: 1.1216 - accuracy: 0.5482\n",
            "Epoch 3: val_loss improved from 1.28549 to 1.26667, saving model to best_cnn_model.h5\n",
            "1125/1125 [==============================] - 11s 10ms/step - loss: 1.1217 - accuracy: 0.5481 - val_loss: 1.2667 - val_accuracy: 0.4975\n",
            "Epoch 4/20\n",
            "1123/1125 [============================>.] - ETA: 0s - loss: 1.0269 - accuracy: 0.5856\n",
            "Epoch 4: val_loss improved from 1.26667 to 1.23663, saving model to best_cnn_model.h5\n",
            "1125/1125 [==============================] - 11s 10ms/step - loss: 1.0265 - accuracy: 0.5859 - val_loss: 1.2366 - val_accuracy: 0.5265\n",
            "Epoch 5/20\n",
            "1121/1125 [============================>.] - ETA: 0s - loss: 0.9261 - accuracy: 0.6307\n",
            "Epoch 5: val_loss improved from 1.23663 to 1.21381, saving model to best_cnn_model.h5\n",
            "1125/1125 [==============================] - 11s 10ms/step - loss: 0.9264 - accuracy: 0.6304 - val_loss: 1.2138 - val_accuracy: 0.5255\n",
            "Epoch 6/20\n",
            "1124/1125 [============================>.] - ETA: 0s - loss: 0.8192 - accuracy: 0.6817\n",
            "Epoch 6: val_loss did not improve from 1.21381\n",
            "1125/1125 [==============================] - 11s 10ms/step - loss: 0.8196 - accuracy: 0.6815 - val_loss: 1.3054 - val_accuracy: 0.5155\n",
            "Epoch 7/20\n",
            "1122/1125 [============================>.] - ETA: 0s - loss: 0.7333 - accuracy: 0.7149\n",
            "Epoch 7: val_loss did not improve from 1.21381\n",
            "1125/1125 [==============================] - 11s 10ms/step - loss: 0.7340 - accuracy: 0.7146 - val_loss: 1.3052 - val_accuracy: 0.5310\n",
            "Epoch 8/20\n",
            "1122/1125 [============================>.] - ETA: 0s - loss: 0.6576 - accuracy: 0.7512\n",
            "Epoch 8: val_loss did not improve from 1.21381\n",
            "1125/1125 [==============================] - 10s 9ms/step - loss: 0.6575 - accuracy: 0.7511 - val_loss: 1.4049 - val_accuracy: 0.5370\n",
            "Epoch 9/20\n",
            "1124/1125 [============================>.] - ETA: 0s - loss: 0.5754 - accuracy: 0.7855\n",
            "Epoch 9: val_loss did not improve from 1.21381\n",
            "1125/1125 [==============================] - 11s 9ms/step - loss: 0.5755 - accuracy: 0.7856 - val_loss: 1.4325 - val_accuracy: 0.5355\n",
            "Epoch 10/20\n",
            "1124/1125 [============================>.] - ETA: 0s - loss: 0.4965 - accuracy: 0.8126\n",
            "Epoch 10: val_loss did not improve from 1.21381\n",
            "1125/1125 [==============================] - 11s 10ms/step - loss: 0.4965 - accuracy: 0.8126 - val_loss: 1.6089 - val_accuracy: 0.5315\n",
            "Epoch 11/20\n",
            "1122/1125 [============================>.] - ETA: 0s - loss: 0.4543 - accuracy: 0.8313\n",
            "Epoch 11: val_loss did not improve from 1.21381\n",
            "1125/1125 [==============================] - 11s 10ms/step - loss: 0.4550 - accuracy: 0.8313 - val_loss: 1.5891 - val_accuracy: 0.5395\n",
            "Epoch 12/20\n",
            "1125/1125 [==============================] - ETA: 0s - loss: 0.3973 - accuracy: 0.8525\n",
            "Epoch 12: val_loss did not improve from 1.21381\n",
            "1125/1125 [==============================] - 10s 9ms/step - loss: 0.3973 - accuracy: 0.8525 - val_loss: 1.5906 - val_accuracy: 0.5610\n",
            "Epoch 13/20\n",
            "1125/1125 [==============================] - ETA: 0s - loss: 0.3692 - accuracy: 0.8650\n",
            "Epoch 13: val_loss did not improve from 1.21381\n",
            "1125/1125 [==============================] - 11s 10ms/step - loss: 0.3692 - accuracy: 0.8650 - val_loss: 1.6752 - val_accuracy: 0.5355\n",
            "Epoch 14/20\n",
            "1123/1125 [============================>.] - ETA: 0s - loss: 0.3332 - accuracy: 0.8809\n",
            "Epoch 14: val_loss did not improve from 1.21381\n",
            "1125/1125 [==============================] - 11s 10ms/step - loss: 0.3333 - accuracy: 0.8808 - val_loss: 1.7954 - val_accuracy: 0.5455\n",
            "Epoch 15/20\n",
            "1123/1125 [============================>.] - ETA: 0s - loss: 0.3156 - accuracy: 0.8904\n",
            "Epoch 15: val_loss did not improve from 1.21381\n",
            "1125/1125 [==============================] - 10s 9ms/step - loss: 0.3153 - accuracy: 0.8905 - val_loss: 1.7473 - val_accuracy: 0.5635\n",
            "Epoch 16/20\n",
            "1121/1125 [============================>.] - ETA: 0s - loss: 0.2716 - accuracy: 0.9058\n",
            "Epoch 16: val_loss did not improve from 1.21381\n",
            "1125/1125 [==============================] - 11s 9ms/step - loss: 0.2715 - accuracy: 0.9059 - val_loss: 1.9802 - val_accuracy: 0.5415\n",
            "Epoch 17/20\n",
            "1124/1125 [============================>.] - ETA: 0s - loss: 0.2492 - accuracy: 0.9125\n",
            "Epoch 17: val_loss did not improve from 1.21381\n",
            "1125/1125 [==============================] - 11s 10ms/step - loss: 0.2491 - accuracy: 0.9126 - val_loss: 1.8464 - val_accuracy: 0.5500\n",
            "Epoch 18/20\n",
            "1125/1125 [==============================] - ETA: 0s - loss: 0.2335 - accuracy: 0.9211\n",
            "Epoch 18: val_loss did not improve from 1.21381\n",
            "1125/1125 [==============================] - 11s 9ms/step - loss: 0.2335 - accuracy: 0.9211 - val_loss: 2.1529 - val_accuracy: 0.5450\n",
            "Epoch 19/20\n",
            "1121/1125 [============================>.] - ETA: 0s - loss: 0.2223 - accuracy: 0.9243\n",
            "Epoch 19: val_loss did not improve from 1.21381\n",
            "1125/1125 [==============================] - 10s 9ms/step - loss: 0.2224 - accuracy: 0.9243 - val_loss: 2.0545 - val_accuracy: 0.5500\n",
            "Epoch 20/20\n",
            "1121/1125 [============================>.] - ETA: 0s - loss: 0.2002 - accuracy: 0.9300\n",
            "Epoch 20: val_loss did not improve from 1.21381\n",
            "1125/1125 [==============================] - 11s 10ms/step - loss: 0.2002 - accuracy: 0.9299 - val_loss: 2.0794 - val_accuracy: 0.5495\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "60FJChkPc7iY"
      },
      "source": [
        "compare_learning(mlp_history, lm_history, cnn_history, vgg_history)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "save_path = F\"/content/gdrive/My Drive/cnn_model.zip\"\n",
        "\n",
        "tf.keras.models.save_model(cnn_model,'cnn_model')\n",
        "import zipfile\n",
        "\n",
        "import os\n",
        "import zipfile\n",
        "\n",
        "def zipdir(path, ziph):\n",
        "    for root, dirs, files in os.walk(path):\n",
        "        for file in files:\n",
        "            ziph.write(os.path.join(root, file))\n",
        "\n",
        "\n",
        "zipf = zipfile.ZipFile(save_path, 'w', zipfile.ZIP_DEFLATED)\n",
        "zipdir('cnn_model', zipf)\n",
        "zipf.close()"
      ],
      "metadata": {
        "id": "lXBvezKStgLA"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}